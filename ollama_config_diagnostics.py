#!/usr/bin/env python3
"""
è¿œç¨‹Ollamaé…ç½®è¯Šæ–­å·¥å…·
ç”¨äºè¯Šæ–­å’Œä¿®å¤è¿œç¨‹OllamaæœåŠ¡é…ç½®é—®é¢˜
"""

import os
import sys
import logging
import requests
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dotenv import load_dotenv

# è®¾ç½®é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.absolute()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class OllamaConfigDiagnostics:
    """è¿œç¨‹Ollamaé…ç½®è¯Šæ–­å·¥å…·"""
    
    def __init__(self):
        self.remote_host = 'http://120.232.79.82:11434'
        self.required_models = ['bge-m3:latest', 'llama3.2:latest']
        self.timeout = 30
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def run_full_diagnosis(self) -> Dict:
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("ğŸ” å¼€å§‹è¿œç¨‹Ollamaé…ç½®å…¨é¢è¯Šæ–­")
        print("=" * 60)
        
        # è¯Šæ–­ç»“æœ
        results = {
            'overall_status': 'unknown',
            'checks': {},
            'recommendations': [],
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # 1. ç¯å¢ƒå˜é‡æ£€æŸ¥
        print("\nğŸ“‹ 1. ç¯å¢ƒå˜é‡é…ç½®æ£€æŸ¥")
        env_result = self.check_environment_variables()
        results['checks']['environment'] = env_result
        self.print_check_result("ç¯å¢ƒå˜é‡é…ç½®", env_result)
        
        # 2. ç½‘ç»œè¿æ¥æ£€æŸ¥
        print("\nğŸŒ 2. ç½‘ç»œè¿æ¥æ£€æŸ¥")
        network_result = self.check_network_connectivity()
        results['checks']['network'] = network_result
        self.print_check_result("ç½‘ç»œè¿æ¥", network_result)
        
        # 3. OllamaæœåŠ¡æ£€æŸ¥
        print("\nğŸ› ï¸ 3. OllamaæœåŠ¡å¯ç”¨æ€§æ£€æŸ¥")
        service_result = self.check_ollama_service()
        results['checks']['service'] = service_result
        self.print_check_result("OllamaæœåŠ¡", service_result)
        
        # 4. æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥
        print("\nğŸ¤– 4. æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥")
        model_result = self.check_model_availability()
        results['checks']['models'] = model_result
        self.print_check_result("æ¨¡å‹å¯ç”¨æ€§", model_result)
        
        # 5. åŠŸèƒ½æµ‹è¯•
        print("\nâš¡ 5. åŠŸèƒ½æµ‹è¯•")
        function_result = self.test_ollama_functions()
        results['checks']['functions'] = function_result
        self.print_check_result("åŠŸèƒ½æµ‹è¯•", function_result)
        
        # 6. æœ¬åœ°é…ç½®å†²çªæ£€æŸ¥
        print("\nğŸ”§ 6. æœ¬åœ°é…ç½®å†²çªæ£€æŸ¥")
        conflict_result = self.check_local_conflicts()
        results['checks']['conflicts'] = conflict_result
        self.print_check_result("æœ¬åœ°é…ç½®å†²çª", conflict_result)
        
        # è®¡ç®—æ€»ä½“çŠ¶æ€
        results['overall_status'] = self.calculate_overall_status(results['checks'])
        
        # ç”Ÿæˆå»ºè®®
        results['recommendations'] = self.generate_recommendations(results['checks'])
        
        # æ˜¾ç¤ºæ€»ç»“
        self.print_summary(results)
        
        return results
    
    def check_environment_variables(self) -> Dict:
        """æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®"""
        required_configs = {
            'LLM_BINDING_HOST': self.remote_host,
            'OLLAMA_HOST': self.remote_host,
            'OLLAMA_BASE_URL': self.remote_host,
            'OLLAMA_NO_SERVE': '1',
            'EMBEDDING_MODEL': 'bge-m3:latest',
            'LLM_MODEL': 'llama3.2:latest'
        }
        
        results = {
            'success': True,
            'details': {},
            'issues': []
        }
        
        for key, expected in required_configs.items():
            current = os.environ.get(key)
            if current == expected:
                results['details'][key] = {'status': 'correct', 'value': current}
                print(f"   âœ… {key}: {current}")
            else:
                results['details'][key] = {
                    'status': 'incorrect', 
                    'current': current, 
                    'expected': expected
                }
                results['issues'].append(f"{key} é…ç½®é”™è¯¯: {current} (æœŸæœ›: {expected})")
                results['success'] = False
                print(f"   âŒ {key}: {current} (æœŸæœ›: {expected})")
        
        # æ£€æŸ¥å¯ç–‘çš„æœ¬åœ°é…ç½®
        suspicious_patterns = ['127.0.0.1', 'localhost', ':64482']
        for key in ['LLM_BINDING_HOST', 'OLLAMA_HOST', 'OLLAMA_BASE_URL']:
            value = os.environ.get(key, '')
            for pattern in suspicious_patterns:
                if pattern in value:
                    results['issues'].append(f"æ£€æµ‹åˆ°å¯ç–‘çš„æœ¬åœ°é…ç½®: {key}={value}")
                    results['success'] = False
                    print(f"   âš ï¸ å¯ç–‘é…ç½®: {key}={value}")
        
        return results
    
    def check_network_connectivity(self) -> Dict:
        """æ£€æŸ¥ç½‘ç»œè¿æ¥"""
        try:
            print(f"   ğŸ”— æµ‹è¯•è¿æ¥: {self.remote_host}")
            response = requests.get(f"{self.remote_host}/api/version", timeout=10)
            
            if response.status_code == 200:
                version_data = response.json()
                return {
                    'success': True,
                    'response_time': response.elapsed.total_seconds(),
                    'version': version_data.get('version', 'unknown'),
                    'details': f"è¿æ¥æˆåŠŸï¼Œå“åº”æ—¶é—´: {response.elapsed.total_seconds():.2f}ç§’"
                }
            else:
                return {
                    'success': False,
                    'error': f"HTTP {response.status_code}",
                    'details': f"æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}"
                }
        except requests.exceptions.Timeout:
            return {
                'success': False,
                'error': 'timeout',
                'details': 'è¿æ¥è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–æœåŠ¡ä¸å¯ç”¨'
            }
        except requests.exceptions.ConnectionError as e:
            return {
                'success': False,
                'error': 'connection_error',
                'details': f'è¿æ¥å¤±è´¥: {str(e)}'
            }
        except Exception as e:
            return {
                'success': False,
                'error': 'unknown',
                'details': f'æœªçŸ¥é”™è¯¯: {str(e)}'
            }
    
    def check_ollama_service(self) -> Dict:
        """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
        try:
            # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
            response = requests.get(f"{self.remote_host}/api/tags", timeout=self.timeout)
            
            if response.status_code == 200:
                data = response.json()
                models = data.get('models', [])
                
                return {
                    'success': True,
                    'model_count': len(models),
                    'models': [m.get('name', '') for m in models],
                    'details': f"æœåŠ¡æ­£å¸¸ï¼Œå·²å®‰è£… {len(models)} ä¸ªæ¨¡å‹"
                }
            else:
                return {
                    'success': False,
                    'error': f"HTTP {response.status_code}",
                    'details': f"æœåŠ¡çŠ¶æ€å¼‚å¸¸: {response.status_code}"
                }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'details': f'æœåŠ¡æ£€æŸ¥å¤±è´¥: {str(e)}'
            }
    
    def check_model_availability(self) -> Dict:
        """æ£€æŸ¥æ‰€éœ€æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(f"{self.remote_host}/api/tags", timeout=self.timeout)
            
            if response.status_code != 200:
                return {
                    'success': False,
                    'error': f"HTTP {response.status_code}",
                    'details': 'æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨'
                }
            
            data = response.json()
            available_models = [m.get('name', '') for m in data.get('models', [])]
            
            results = {
                'success': True,
                'available_models': available_models,
                'model_status': {},
                'missing_models': []
            }
            
            for required_model in self.required_models:
                is_available = any(required_model in model for model in available_models)
                results['model_status'][required_model] = is_available
                
                if is_available:
                    print(f"   âœ… {required_model}: å¯ç”¨")
                else:
                    print(f"   âŒ {required_model}: ä¸å¯ç”¨")
                    results['missing_models'].append(required_model)
                    results['success'] = False
            
            return results
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'details': f'æ¨¡å‹æ£€æŸ¥å¤±è´¥: {str(e)}'
            }
    
    def test_ollama_functions(self) -> Dict:
        """æµ‹è¯•Ollamaæ ¸å¿ƒåŠŸèƒ½"""
        results = {
            'success': True,
            'tests': {},
            'details': []
        }
        
        # æµ‹è¯•åµŒå…¥åŠŸèƒ½
        embedding_result = self.test_embedding_function()
        results['tests']['embedding'] = embedding_result
        if not embedding_result['success']:
            results['success'] = False
        
        # æµ‹è¯•ç”ŸæˆåŠŸèƒ½
        generation_result = self.test_generation_function()
        results['tests']['generation'] = generation_result
        if not generation_result['success']:
            results['success'] = False
        
        return results
    
    def test_embedding_function(self) -> Dict:
        """æµ‹è¯•åµŒå…¥åŠŸèƒ½"""
        try:
            print("   ğŸ§ª æµ‹è¯•åµŒå…¥åŠŸèƒ½...")
            payload = {
                "model": "bge-m3:latest",
                "prompt": "æµ‹è¯•æ–‡æœ¬"
            }
            
            response = requests.post(
                f"{self.remote_host}/api/embeddings", 
                json=payload, 
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'embedding' in result and result['embedding']:
                    embedding_dim = len(result['embedding'])
                    print(f"      âœ… åµŒå…¥åŠŸèƒ½æ­£å¸¸ï¼Œå‘é‡ç»´åº¦: {embedding_dim}")
                    return {
                        'success': True,
                        'embedding_dimension': embedding_dim,
                        'response_time': response.elapsed.total_seconds()
                    }
                else:
                    print("      âŒ åµŒå…¥åŠŸèƒ½å¼‚å¸¸ï¼šè¿”å›ç»“æœæ ¼å¼é”™è¯¯")
                    return {
                        'success': False,
                        'error': 'è¿”å›ç»“æœæ ¼å¼é”™è¯¯'
                    }
            else:
                print(f"      âŒ åµŒå…¥åŠŸèƒ½å¼‚å¸¸ï¼šHTTP {response.status_code}")
                return {
                    'success': False,
                    'error': f"HTTP {response.status_code}"
                }
                
        except Exception as e:
            print(f"      âŒ åµŒå…¥åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def test_generation_function(self) -> Dict:
        """æµ‹è¯•æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
        try:
            print("   ğŸ§ª æµ‹è¯•æ–‡æœ¬ç”ŸæˆåŠŸèƒ½...")
            payload = {
                "model": "llama3.2:latest",
                "prompt": "ç®€å•å›ç­”ï¼š1+1ç­‰äºå¤šå°‘ï¼Ÿ",
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "max_tokens": 50
                }
            }
            
            response = requests.post(
                f"{self.remote_host}/api/generate", 
                json=payload, 
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'response' in result and result['response']:
                    generated_text = result['response'].strip()
                    print(f"      âœ… æ–‡æœ¬ç”ŸæˆåŠŸèƒ½æ­£å¸¸ï¼Œç”Ÿæˆå†…å®¹: {generated_text[:50]}...")
                    return {
                        'success': True,
                        'generated_text': generated_text,
                        'response_time': response.elapsed.total_seconds()
                    }
                else:
                    print("      âŒ æ–‡æœ¬ç”ŸæˆåŠŸèƒ½å¼‚å¸¸ï¼šè¿”å›ç»“æœæ ¼å¼é”™è¯¯")
                    return {
                        'success': False,
                        'error': 'è¿”å›ç»“æœæ ¼å¼é”™è¯¯'
                    }
            else:
                print(f"      âŒ æ–‡æœ¬ç”ŸæˆåŠŸèƒ½å¼‚å¸¸ï¼šHTTP {response.status_code}")
                return {
                    'success': False,
                    'error': f"HTTP {response.status_code}"
                }
                
        except Exception as e:
            print(f"      âŒ æ–‡æœ¬ç”ŸæˆåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def check_local_conflicts(self) -> Dict:
        """æ£€æŸ¥æœ¬åœ°é…ç½®å†²çª"""
        conflicts = []
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­çš„æœ¬åœ°åœ°å€
        local_patterns = ['127.0.0.1', 'localhost', ':64482', ':11434']
        env_vars_to_check = [
            'LLM_BINDING_HOST', 'OLLAMA_HOST', 'OLLAMA_BASE_URL',
            'OLLAMA_API_BASE', 'OLLAMA_ENDPOINT'
        ]
        
        for var in env_vars_to_check:
            value = os.environ.get(var, '')
            for pattern in local_patterns:
                if pattern in value and 'http://120.232.79.82' not in value:
                    conflicts.append(f"ç¯å¢ƒå˜é‡ {var} åŒ…å«æœ¬åœ°åœ°å€æ¨¡å¼: {value}")
        
        # æ£€æŸ¥OLLAMA_NO_SERVEè®¾ç½®
        no_serve = os.environ.get('OLLAMA_NO_SERVE', '')
        if no_serve != '1':
            conflicts.append(f"OLLAMA_NO_SERVE æœªæ­£ç¡®è®¾ç½®ä¸º '1'ï¼Œå½“å‰å€¼: '{no_serve}'")
        
        if conflicts:
            for conflict in conflicts:
                print(f"   âš ï¸ {conflict}")
            return {
                'success': False,
                'conflicts': conflicts,
                'details': f"å‘ç° {len(conflicts)} ä¸ªé…ç½®å†²çª"
            }
        else:
            print("   âœ… æœªå‘ç°æœ¬åœ°é…ç½®å†²çª")
            return {
                'success': True,
                'conflicts': [],
                'details': "æœªå‘ç°é…ç½®å†²çª"
            }
    
    def print_check_result(self, check_name: str, result: Dict):
        """æ‰“å°æ£€æŸ¥ç»“æœ"""
        status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
        details = result.get('details', '')
        print(f"   çŠ¶æ€: {status}")
        if details:
            print(f"   è¯¦æƒ…: {details}")
    
    def calculate_overall_status(self, checks: Dict) -> str:
        """è®¡ç®—æ€»ä½“çŠ¶æ€"""
        critical_checks = ['network', 'service', 'models']
        critical_passed = all(checks.get(check, {}).get('success', False) for check in critical_checks)
        
        if critical_passed:
            return 'healthy'
        else:
            return 'unhealthy'
    
    def generate_recommendations(self, checks: Dict) -> List[str]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        # ç¯å¢ƒå˜é‡å»ºè®®
        if not checks.get('environment', {}).get('success', False):
            recommendations.append("ä¿®å¤ç¯å¢ƒå˜é‡é…ç½®ï¼šè¿è¡Œ force_remote_ollama_config() å‡½æ•°")
        
        # ç½‘ç»œè¿æ¥å»ºè®®
        if not checks.get('network', {}).get('success', False):
            recommendations.append("æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼šç¡®ä¿å¯ä»¥è®¿é—® http://120.232.79.82:11434")
        
        # æ¨¡å‹å»ºè®®
        model_check = checks.get('models', {})
        if not model_check.get('success', False):
            missing = model_check.get('missing_models', [])
            if missing:
                recommendations.append(f"å®‰è£…ç¼ºå¤±çš„æ¨¡å‹ï¼š{', '.join(missing)}")
        
        # åŠŸèƒ½æµ‹è¯•å»ºè®®
        function_check = checks.get('functions', {})
        if not function_check.get('success', False):
            recommendations.append("OllamaåŠŸèƒ½å¼‚å¸¸ï¼Œæ£€æŸ¥æœåŠ¡çŠ¶æ€å’Œæ¨¡å‹å®‰è£…")
        
        # é…ç½®å†²çªå»ºè®®
        conflict_check = checks.get('conflicts', {})
        if not conflict_check.get('success', False):
            recommendations.append("è§£å†³é…ç½®å†²çªï¼šæ¸…ç†æœ¬åœ°Ollamaé…ç½®")
        
        return recommendations
    
    def print_summary(self, results: Dict):
        """æ‰“å°è¯Šæ–­æ€»ç»“"""
        print("\n" + "=" * 60)
        print("ğŸ¯ è¯Šæ–­æ€»ç»“")
        print("=" * 60)
        
        status = results['overall_status']
        if status == 'healthy':
            print("âœ… æ€»ä½“çŠ¶æ€: å¥åº· - è¿œç¨‹Ollamaé…ç½®æ­£å¸¸")
        else:
            print("âŒ æ€»ä½“çŠ¶æ€: å¼‚å¸¸ - éœ€è¦ä¿®å¤é…ç½®é—®é¢˜")
        
        recommendations = results.get('recommendations', [])
        if recommendations:
            print("\nğŸ”§ ä¿®å¤å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        else:
            print("\nğŸ‰ æ— éœ€é¢å¤–ä¿®å¤ï¼Œé…ç½®å·²æ­£ç¡®ï¼")
        
        print(f"\nğŸ“… è¯Šæ–­æ—¶é—´: {results['timestamp']}")
    
    def force_fix_configuration(self):
        """å¼ºåˆ¶ä¿®å¤é…ç½®"""
        print("ğŸ”§ å¼€å§‹å¼ºåˆ¶ä¿®å¤è¿œç¨‹Ollamaé…ç½®...")
        
        config_vars = {
            'LLM_BINDING_HOST': self.remote_host,
            'OLLAMA_HOST': self.remote_host,
            'OLLAMA_BASE_URL': self.remote_host,
            'OLLAMA_NO_SERVE': '1',
            'OLLAMA_ORIGINS': '*',
            'OLLAMA_KEEP_ALIVE': '5m',
            'EMBEDDING_MODEL': 'bge-m3:latest',
            'LLM_MODEL': 'llama3.2:latest',
            'LLM_BINDING': 'ollama'
        }
        
        for key, value in config_vars.items():
            old_value = os.environ.get(key)
            os.environ[key] = value
            if old_value != value:
                print(f"   âœ… {key}: {old_value} -> {value}")
            else:
                print(f"   âœ“ {key}: {value}")
        
        print("âœ… é…ç½®ä¿®å¤å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” è¿œç¨‹Ollamaé…ç½®è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    diagnostics = OllamaConfigDiagnostics()
    
    # è¿è¡Œè¯Šæ–­
    results = diagnostics.run_full_diagnosis()
    
    # å¦‚æœæ£€æµ‹åˆ°é—®é¢˜ï¼Œè¯¢é—®æ˜¯å¦è‡ªåŠ¨ä¿®å¤
    if results['overall_status'] != 'healthy':
        print("\nâ“ æ£€æµ‹åˆ°é…ç½®é—®é¢˜ï¼Œæ˜¯å¦è‡ªåŠ¨ä¿®å¤ï¼Ÿ")
        choice = input("è¾“å…¥ 'y' è‡ªåŠ¨ä¿®å¤ï¼Œå…¶ä»–é”®è·³è¿‡: ").strip().lower()
        
        if choice == 'y':
            diagnostics.force_fix_configuration()
            print("\nğŸ”„ é‡æ–°è¿è¡Œè¯Šæ–­éªŒè¯ä¿®å¤æ•ˆæœ...")
            diagnostics.run_full_diagnosis()
    
    return results


if __name__ == "__main__":
    main()