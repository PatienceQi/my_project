# 技术架构

<cite>
**本文档引用的文件**  
- [api_server.py](file://backend/api_server.py) - *新增GraphRAG增强API端点*
- [graphrag_engine.py](file://backend/graphrag_engine.py) - *GraphRAG核心引擎实现*
- [vector_retrieval.py](file://backend/vector_retrieval.py) - *基于Chroma的向量检索实现*
- [graph_query.py](file://backend/graph_query.py) - *基于Neo4j的图谱查询实现*
- [entity_extractor.py](file://backend/entity_extractor.py) - *实体提取器实现*
- [hallucination_detector.py](file://backend/hallucination_detector.py) - *幻觉检测器实现*
- [index_graphrag.html](file://frontend/index_graphrag.html) - *GraphRAG增强版前端界面*
- [requirements.txt](file://requirements.txt) - *新增chromadb、numpy等依赖*
- [import_policy_data.py](file://scripts/import_policy_data.py)
- [test_backend_response.py](file://scripts/test_backend_response.py)
- [test_ollama_connection.py](file://scripts/test_ollama_connection.py)
- [test_neo4j_connection.py](file://scripts/test_neo4j_connection.py)
- [README.md](file://README.md)
- [[OCR]_华侨经济文化合作试验区.json](file://database/[OCR]_华侨经济文化合作试验区.json)
- [connections.py](file://backend/connections.py) - *更新连接管理与超时策略*
- [ollama_error_handler.py](file://backend/ollama_error_handler.py) - *更新Ollama错误处理机制*
- [health_checker.py](file://backend/health_checker.py) - *系统健康检查模块*
</cite>

## 更新摘要
**变更内容**   
- 新增GraphRAG技术栈的详细说明，包括chromadb、numpy等新依赖
- 新增GraphRAG核心组件分析，涵盖graphrag_engine.py及关联模块
- 新增GraphRAG系统架构图和组件交互图
- 新增前后端交互流程中GraphRAG增强API的说明
- 更新依赖服务集成部分，增加向量数据库Chroma的集成说明
- 新增幻觉检测机制的架构说明
- 更新部署与配置说明以反映新架构
- **新增连接管理与超时策略的详细说明，反映connections.py和ollama_error_handler.py的更新**
- **新增系统健康检查机制的详细描述，包括基础、深度健康检查及历史记录功能**

## 目录
1. [项目结构](#项目结构)
2. [系统架构概述](#系统架构概述)
3. [核心组件分析](#核心组件分析)
4. [前后端交互流程](#前后端交互流程)
5. [数据处理与导入机制](#数据处理与导入机制)
6. [依赖服务集成](#依赖服务集成)
7. [错误处理与连接验证](#错误处理与连接验证)
8. [部署与配置说明](#部署与配置说明)

## 项目结构

本项目采用清晰的分层架构，将前端、后端、数据与脚本分离，便于维护与扩展。

```
graph TB
subgraph "根目录"
A[项目根目录]
end
subgraph "backend"
B[api_server.py]
C[graphrag_engine.py]
D[vector_retrieval.py]
E[graph_query.py]
F[entity_extractor.py]
G[hallucination_detector.py]
end
subgraph "database"
H[[政策法规数据.json]]
end
subgraph "frontend"
I[index.html]
J[index_graphrag.html]
end
subgraph "scripts"
K[import_policy_data.py]
L[test_backend_response.py]
M[test_neo4j_connection.py]
N[test_ollama_connection.py]
end
A --> B
A --> C
A --> D
A --> E
A --> F
A --> G
A --> H
A --> I
A --> J
A --> K
A --> L
A --> M
A --> N
```

**图示来源**  
- [README.md](file://README.md#L100-L150)

## 系统架构概述

本系统采用先进的GraphRAG（图谱增强检索生成）架构，结合图数据库、向量数据库与大语言模型，实现对政策法规的智能问答。整体架构分为前端、后端、图数据库（Neo4j）、向量数据库（Chroma）和大模型服务（Ollama）五大组件。

```
graph LR
A[前端用户界面] --> |HTTP POST| B[Flask后端API]
B --> |Cypher查询| C[Neo4j图数据库]
B --> |向量查询| D[Chroma向量数据库]
C --> |图谱结果| B
D --> |向量结果| B
B --> |提示词构建| E[Ollama大模型]
E --> |生成答案| B
B --> |JSON响应| A
```

**图示来源**  
- [api_server.py](file://backend/api_server.py#L1-L120)
- [index_graphrag.html](file://frontend/index_graphrag.html#L1-L254)

## 核心组件分析

### 后端API服务

后端使用Flask框架构建RESTful API，接收前端问题，调用Neo4j进行图谱检索，调用Chroma进行向量检索，并通过Ollama生成最终答案。

**功能流程**：
1. 接收用户问题
2. 在Neo4j中检索相关政策、章节、子章节及实体关系
3. 在Chroma中进行语义向量检索
4. 构建增强上下文提示词
5. 调用Ollama生成回答
6. 通过幻觉检测器验证答案可靠性
7. 返回包含可信度评估的结构化响应

```python
@app.route('/api/ask/enhanced', methods=['POST'])
def ask_enhanced():
    if not GRAPHRAG_AVAILABLE or not graphrag_engine:
        return jsonify({"error": "GraphRAG功能不可用"}), 503
    
    data = request.get_json()
    question = data.get('question', '').strip()
    
    result = graphrag_engine.answer_question(
        question, 
        use_graph=True, 
        return_confidence=True
    )
    
    return jsonify(result)
```

**代码来源**  
- [api_server.py](file://backend/api_server.py#L453-L550)

### GraphRAG引擎

GraphRAG引擎是系统的核心，整合了向量检索、图谱查询、实体提取和幻觉检测四大模块，提供统一的问答接口。

**核心组件**：
- **向量检索器**：基于Chroma和远程Ollama的bge-m3嵌入模型
- **图谱查询引擎**：基于Neo4j的知识图谱查询
- **实体提取器**：基于Ollama的智能实体识别
- **幻觉检测器**：基于多维度验证的答案可信度评估

```python
class GraphRAGEngine:
    def __init__(self):
        self.vector_retriever = VectorRetriever()
        self.graph_query_engine = GraphQueryEngine()
        self.entity_extractor = EntityExtractor()
        self.hallucination_detector = HallucinationDetector(
            self.graph_query_engine, 
            self.entity_extractor
        )
    
    def answer_question(self, question: str, use_graph: bool = True, 
                       return_confidence: bool = True) -> Dict:
        # 1. 实体提取
        question_entities = self.entity_extractor.extract_entities_from_question(question)
        
        # 2. 并行检索
        vector_results = self.vector_retriever.search(question, top_k=5)
        graph_context = self._query_graph_context(question_entities)
        
        # 3. 构建增强上下文
        enhanced_context = self._build_enhanced_context(
            question, question_entities, vector_results, graph_context
        )
        
        # 4. 生成答案
        answer = self._generate_answer(question, enhanced_context)
        
        # 5. 幻觉检测
        confidence_info = self.hallucination_detector.detect_hallucination(
            answer, question, vector_results, graph_context
        )
        
        return {**response, **confidence_info}
```

**代码来源**  
- [graphrag_engine.py](file://backend/graphrag_engine.py#L0-L199)

### 前端用户界面

前端为单页应用，使用原生HTML/CSS/JavaScript实现，提供GraphRAG增强版的聊天式问答界面。

**主要功能**：
- 实时显示连接状态和系统健康状况
- 用户输入问题并发送
- 显示AI回答、可信度评估、警告信息和引用来源
- 支持会话管理和问答对比功能
- 提供系统状态监控

```javascript
async function sendMessage() {
    const useGraph = document.getElementById('useGraphOption').checked;
    const returnConfidence = document.getElementById('returnConfidenceOption').checked;
    
    const response = await fetch(ASK_ENHANCED_URL, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
            question: userInput, 
            use_graph: useGraph,
            return_confidence: returnConfidence,
            session_id: currentSessionId
        })
    });
    
    const data = await response.json();
    displayResponse(data);
}
```

**代码来源**  
- [index_graphrag.html](file://frontend/index_graphrag.html#L400-L600)

## 前后端交互流程

系统采用HTTP协议进行前后端通信，遵循REST风格，数据格式为JSON。GraphRAG架构支持传统RAG和增强RAG两种模式。

```
sequenceDiagram
participant 用户
participant 前端 as 前端(index_graphrag.html)
participant 后端 as 后端(api_server.py)
participant Neo4j
participant Chroma
participant Ollama
用户->>前端 : 输入问题并发送
前端->>后端 : POST /api/ask/enhanced {question : "...", use_graph : true}
后端->>Neo4j : 执行Cypher查询实体关系
后端->>Chroma : 执行向量相似度检索
Neo4j-->>后端 : 返回实体关系网络
Chroma-->>后端 : 返回语义匹配文档
后端->>Ollama : 发送增强提示词请求
Ollama-->>后端 : 返回生成的回答
后端->>后端 : 执行幻觉检测
后端-->>前端 : 返回{answer, confidence, warnings, sources}
前端-->>用户 : 显示回答、可信度和引用信息
```

**图示来源**  
- [api_server.py](file://backend/api_server.py#L450-L649)
- [index_graphrag.html](file://frontend/index_graphrag.html#L400-L600)

## 数据处理与导入机制

系统通过`import_policy_data.py`脚本将JSON格式的政策法规数据导入Neo4j，并利用大模型进行实体识别与关系抽取。

### 数据结构示例

```json
{
  "title": "汕华管委规",
  "main_body": [
    {
      "section_title": "第一章总则",
      "content": "第一条为贯彻市委、市政府决策部署..."
    }
  ]
}
```

### 导入流程

```
flowchart TD
A[读取JSON文件] --> B[创建Policy节点]
B --> C[创建Section节点]
C --> D[调用Ollama提取实体]
D --> E[创建Entity节点]
E --> F[建立实体间关系]
F --> G[分块生成向量]
G --> H[存储到Chroma]
H --> I[输出导入摘要]
```

**代码来源**  
- [import_policy_data.py](file://scripts/import_policy_data.py#L1-L574)
- [[OCR]_华侨经济文化合作试验区.json](file://database/[OCR]_华侨经济文化合作试验区.json#L1-L39)

## 依赖服务集成

### Neo4j图数据库集成

使用`neo4j` Python驱动连接图数据库，存储政策法规的层次结构与实体关系。

```python
driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))

def query_entities_by_name(self, entity_names: List[str]) -> List[Dict]:
    query = """
    UNWIND $entity_names AS entity_name
    MATCH (e:Entity) 
    WHERE e.name CONTAINS entity_name OR e.text CONTAINS entity_name
    OPTIONAL MATCH (e)-[r]->(related:Entity)
    RETURN DISTINCT 
        e.name as entity_name,
        e.type as entity_type,
        e.text as entity_text,
        collect(DISTINCT {
            relation: type(r),
            target: related.name,
            target_type: related.type
        }) as relations
    LIMIT $limit
    """
    return session.run(query, {'entity_names': entity_names}).data()
```

**代码来源**  
- [graph_query.py](file://backend/graph_query.py#L50-L100)
- [test_neo4j_connection.py](file://scripts/test_neo4j_connection.py#L1-L25)

### Chroma向量数据库集成

使用`chromadb`库集成向量数据库，支持基于bge-m3嵌入模型的语义检索。

```python
class VectorRetriever:
    def __init__(self):
        self.embedding_model_name = os.getenv('EMBEDDING_MODEL', 'bge-m3:latest')
        self.ollama_host = os.getenv('LLM_BINDING_HOST', 'http://120.232.79.82:11434')
        self.persist_dir = os.getenv('CHROMA_PERSIST_DIR', './data/chroma_db')
        
        self.chroma_client = chromadb.PersistentClient(path=self.persist_dir)
        self.collection = self.chroma_client.get_or_create_collection(
            name=f"policy_documents_{self.embedding_model_name.replace(':', '_')}"
        )
    
    def _get_embedding_from_ollama(self, text: str) -> Optional[List[float]]:
        url = f"{self.ollama_host}/api/embed"
        payload = {
            "model": self.embedding_model_name,
            "input": text.strip()
        }
        response = requests.post(url, json=payload, timeout=30)
        return response.json().get('embeddings', [])[0]
```

**代码来源**  
- [vector_retrieval.py](file://backend/vector_retrieval.py#L0-L199)
- [requirements.txt](file://requirements.txt#L10-L12)

### Ollama大模型集成

通过`ollama`库调用本地或远程大模型服务，生成自然语言回答。

```python
client = ollama.Client(host=LLM_BINDING_HOST)
response = client.chat(model=LLM_MODEL, messages=[
    {"role": "user", "content": prompt}
])
```

**代码来源**  
- [api_server.py](file://backend/api_server.py#L15-L20)
- [test_ollama_connection.py](file://scripts/test_ollama_connection.py#L1-L25)

## 错误处理与连接验证

系统提供多个测试脚本，用于验证各组件的连接状态。

### 连接测试脚本

| 脚本文件 | 功能 |
|--------|------|
| `test_neo4j_connection.py` | 测试Neo4j数据库连接 |
| `test_ollama_connection.py` | 测试Ollama服务连接 |
| `test_backend_response.py` | 测试后端API响应 |
| `test_graphrag_system.py` | 测试GraphRAG系统完整性 |

```python
# test_backend_response.py
response = requests.post('http://127.0.0.1:5000/api/ask', json={'question': '测试'})
print(response.json())
```

**代码来源**  
- [test_backend_response.py](file://scripts/test_backend_response.py#L1-L21)
- [test_ollama_connection.py](file://scripts/test_ollama_connection.py#L1-L25)
- [test_neo4j_connection.py](file://scripts/test_neo4j_connection.py#L1-L25)

### 连接管理与超时策略

系统通过`connections.py`和`ollama_error_handler.py`实现了精细化的连接管理与超时策略，确保服务的稳定性和可靠性。

**核心机制**：
- **Ollama连接管理器**：`OllamaConnectionManager`类强制配置环境变量，确保使用远程Ollama服务，防止意外启动本地服务。
- **超时配置**：关键请求的超时时间已调整为600秒，以适应大模型的长响应时间。
- **健康检查**：集成缓存机制，每分钟最多检查一次服务健康状态，减少网络开销。
- **错误处理**：`OllamaErrorHandler`类移除了本地回退选项，只使用远程服务，并实现了重试机制。

```python
class OllamaConnectionManager:
    def __init__(self, host: str, model: str, timeout: int = 600):
        # 强制设置远程配置
        config_vars = {
            'OLLAMA_HOST': remote_host,
            'OLLAMA_BASE_URL': remote_host,
            'LLM_BINDING_HOST': remote_host,
            'OLLAMA_NO_SERVE': '1',
            'OLLAMA_ORIGINS': '*',
            'OLLAMA_KEEP_ALIVE': '5m'
        }
        # ... 其他代码
```

**代码来源**  
- [connections.py](file://backend/connections.py#L164-L435)
- [ollama_error_handler.py](file://backend/ollama_error_handler.py#L23-L172)

### 系统健康检查机制

系统通过`health_checker.py`模块提供了全面的健康检查功能，包括基础检查、深度检查和历史记录。

**健康检查端点**：
- `/ping`：简单的连接测试
- `/health`：基础健康检查
- `/health/deep`：深度健康检查，包含性能测试
- `/health/history`：健康检查历史
- `/api/uptime`：系统运行时间
- `/api/status`：系统状态综合信息

**检查内容**：
- **连接状态**：检查Neo4j和Ollama服务的连接
- **会话管理**：检查会话数量是否过多
- **系统资源**：监控CPU、内存、磁盘使用率
- **性能测试**：测试数据库查询和LLM服务响应时间

```python
def create_health_endpoints():
    def health():
        health_data = health_checker.get_system_health()
        status_code = 200 if health_data["status"] == "healthy" else 503
        return jsonify(health_data), status_code
    # ... 其他端点
```

**代码来源**  
- [health_checker.py](file://backend/health_checker.py#L0-L352)
- [api_server.py](file://backend/api_server.py#L757-L787)

## 部署与配置说明

### 环境配置

通过`.env`文件配置系统依赖：

```env
NEO4J_URI=neo4j://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
LLM_BINDING_HOST=http://120.232.79.82:11434
LLM_MODEL=llama3.2:latest
EMBEDDING_MODEL=bge-m3:latest
CHROMA_PERSIST_DIR=./data/chroma_db
CONFIDENCE_THRESHOLD=0.4
HALLUCINATION_THRESHOLD=0.7
```

### 部署流程

1. 启动Neo4j数据库
2. 启动Ollama服务
3. 运行`import_policy_data.py`导入数据
4. 启动Flask后端：`python backend/api_server.py`
5. 启动前端：`npm start`

**配置来源**  
- [README.md](file://README.md#L50-L250)
- [requirements.txt](file://requirements.txt#L1-L21)