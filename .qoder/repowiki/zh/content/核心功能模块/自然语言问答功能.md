# 自然语言问答功能

<cite>
**本文档引用的文件**   
- [index.html](file://frontend/index.html) - *已更新以支持会话ID*
- [api_server.py](file://backend/api_server.py) - *已迁移至GraphRAG引擎*
- [graphrag_engine.py](file://backend/graphrag_engine.py) - *新增GraphRAG核心引擎*
- [session_manager.py](file://backend/session_manager.py) - *新增多轮对话支持*
- [entity_extractor.py](file://backend/entity_extractor.py) - *新增实体提取功能*
- [hallucination_detector.py](file://backend/hallucination_detector.py) - *新增幻觉检测功能*
- [vector_retrieval.py](file://backend/vector_retrieval.py) - *新增向量检索功能*
- [requirements.txt](file://requirements.txt)
- [README.md](file://README.md)
- [软件著作权申请.md](file://软件著作权申请.md)
</cite>

## 更新摘要
**变更内容**   
- 将问答核心逻辑从传统RAG迁移至GraphRAG引擎
- 新增多轮对话上下文保持功能
- 新增幻觉检测与可信度评估机制
- 新增向量数据库支持语义检索
- 更新前端以支持会话管理
- 修复Ollama连接配置问题，确保使用远程服务

## 目录
1. [项目结构](#项目结构)
2. [核心组件](#核心组件)
3. [架构概述](#架构概述)
4. [详细组件分析](#详细组件分析)
5. [依赖分析](#依赖分析)
6. [性能考虑](#性能考虑)
7. [故障排除指南](#故障排除指南)

## 项目结构
本项目采用前后端分离的架构，主要由以下模块构成：
- `backend/`：后端API服务，基于Flask框架，负责处理用户请求、与Neo4j数据库交互并调用Ollama大模型。
- `database/`：存储政策法规原始数据的JSON文件。
- `frontend/`：前端用户界面，使用HTML、CSS和JavaScript实现，提供用户提问和答案展示的交互功能。
- `scripts/`：包含数据导入、服务连接测试等辅助脚本。

``mermaid
graph TB
subgraph "前端"
UI["用户界面 (index.html)"]
end
subgraph "后端"
API["API服务 (api_server.py)"]
Neo4j["Neo4j图数据库"]
Ollama["Ollama大模型"]
VectorDB["向量数据库 (Chroma)"]
end
UI --> |POST /api/ask| API
API --> |Cypher查询| Neo4j
API --> |嵌入查询| VectorDB
API --> |生成请求| Ollama
Neo4j --> |返回图谱结果| API
VectorDB --> |返回向量结果| API
Ollama --> |返回生成答案| API
API --> |JSON响应| UI
```

**图示来源**
- [index.html](file://frontend/index.html)
- [api_server.py](file://backend/api_server.py)

**本节来源**
- [README.md](file://README.md#L13-L229)

## 核心组件
系统的核心功能由前端的用户交互组件和后端的问答处理逻辑共同实现。前端负责收集用户问题并展示答案，后端则通过GraphRAG（图谱增强检索生成）架构，结合图数据库的精确检索、向量数据库的语义检索与大模型的语言生成能力，提供准确且可溯源的回答。

**本节来源**
- [软件著作权申请.md](file://软件著作权申请.md#L1-L190)

## 架构概述
系统采用先进的GraphRAG架构，其工作流程如下：
1.  **用户输入**：用户在前端输入自然语言问题。
2.  **请求发送**：前端通过JavaScript将问题以POST请求发送至后端`/api/ask`接口。
3.  **实体提取**：系统首先从问题中提取关键实体（如机构、政策、地点等）。
4.  **混合检索**：并行执行两种检索：
    - **向量检索**：在向量数据库中进行语义相似度搜索。
    - **图谱检索**：在Neo4j图数据库中基于实体关系进行深度查询。
5.  **上下文构建**：将两种检索结果融合，构建增强的上下文。
6.  **答案生成**：将用户问题和增强上下文作为提示词发送给Ollama大模型，生成最终回答。
7.  **幻觉检测**：通过知识图谱验证答案的实体一致性、关系正确性等，评估答案可信度。
8.  **结果返回**：后端将生成的答案、可信度评分、引用来源等信息以JSON格式返回给前端。
9.  **结果展示**：前端解析JSON响应，并在聊天界面中渲染出答案和相关法规列表。

``mermaid
sequenceDiagram
participant User as "用户"
participant Frontend as "前端 (index.html)"
participant Backend as "后端 (api_server.py)"
participant GraphQuery as "图谱查询引擎"
participant VectorRetriever as "向量检索器"
participant Ollama as "Ollama大模型"
participant HallucinationDetector as "幻觉检测器"
User->>Frontend : 输入问题并点击发送
Frontend->>Backend : POST /api/ask {question : "..."}
Backend->>Backend : 提取问题实体
Backend->>VectorRetriever : 语义检索
Backend->>GraphQuery : 图谱查询
VectorRetriever-->>Backend : 返回向量结果
GraphQuery-->>Backend : 返回图谱结果
Backend->>Backend : 构建增强上下文
Backend->>Ollama : 生成回答
Ollama-->>Backend : 返回生成的回答
Backend->>HallucinationDetector : 检测幻觉
HallucinationDetector-->>Backend : 返回可信度评分
Backend-->>Frontend : JSON {answer, confidence, sources, ...}
Frontend->>User : 展示回答和可信度信息
```

**图示来源**
- [index.html](file://frontend/index.html#L130-L253)
- [api_server.py](file://backend/api_server.py#L0-L119)
- [graphrag_engine.py](file://backend/graphrag_engine.py#L0-L485)

**本节来源**
- [index.html](file://frontend/index.html#L130-L253)
- [api_server.py](file://backend/api_server.py#L0-L119)
- [graphrag_engine.py](file://backend/graphrag_engine.py#L0-L485)

## 详细组件分析

### 前端交互分析
前端通过`index.html`中的JavaScript代码实现了完整的用户交互流程。

**事件绑定机制**：
- **发送按钮**：通过`document.getElementById('sendButton').addEventListener('click', sendMessage)`绑定点击事件。
- **回车键**：通过`document.getElementById('userInput').addEventListener('keypress', ...)`监听键盘事件，当按键为Enter时触发`sendMessage`函数。

**提问输入框和发送按钮的HTML结构**：
```html
<div class="input-area">
    <input type="text" id="userInput" placeholder="请输入您的问题..." autocomplete="off">
    <button id="sendButton">发送</button>
</div>
```

**JavaScript发送请求逻辑**：
`sendMessage`函数负责将用户问题通过POST请求发送至后端。其核心代码如下：
```javascript
async function sendMessage() {
    // ... (检查连接状态、获取输入值等)
    
    try {
        const requestBody = { question };
        if (currentSessionId) {
            requestBody.session_id = currentSessionId;
        }
        
        const response = await fetch(ASK_URL, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify(requestBody),
            mode: 'cors'
        });

        const data = await response.json(); // 解析后端返回的JSON
        if (data.session_id && data.session_id !== currentSessionId) {
            currentSessionId = data.session_id;
            sessionInfoElement.textContent = `会话: ${currentSessionId.substring(0, 8)}...`;
        }
        // ... (在页面上展示回答)
    } catch (error) {
        // ... (处理错误)
    }
}
```
该函数使用`fetch` API异步发送请求，并处理响应和可能的错误。现在支持会话ID，以保持多轮对话上下文。

**本节来源**
- [index.html](file://frontend/index.html#L130-L253)

### 后端处理逻辑分析
后端`api_server.py`文件是整个问答功能的核心，实现了从接收请求到生成回答的完整逻辑。

**API端点定义**：
```python
@app.route('/api/ask', methods=['POST'])
def ask():
    data = request.get_json()
    question = data.get('question', '').strip()
    session_id = data.get('session_id')
    
    # 验证输入
    is_valid, error_msg, cleaned_question = InputValidator.validate_question(question)
    if not is_valid:
        return jsonify({'error': error_msg}), 400
        
    # 生成答案
    result = generate_policy_answer(cleaned_question, session_id)
    return jsonify(result)
```
此函数定义了`/api/ask`路由，接收POST请求，解析JSON中的`question`和可选的`session_id`字段，并调用`generate_policy_answer`函数处理。

**GraphRAG协同机制**：
`GraphRAGEngine`类是GraphRAG架构的体现，它整合了多种检索和验证机制。
```python
def answer_question(self, question: str, use_graph: bool = True, return_confidence: bool = True) -> Dict:
    # 1. 提取问题实体
    question_entities = self.entity_extractor.extract_entities_from_question(question)
    
    # 2. 并行执行向量检索和图谱查询
    vector_results = self.vector_retriever.search(question, top_k=5)
    graph_context = {}
    if use_graph and question_entities:
        graph_context = self._query_graph_context(question_entities)
    
    # 3. 构建增强上下文
    enhanced_context = self._build_enhanced_context(question, question_entities, vector_results, graph_context)
    
    # 4. 生成答案
    answer = self._generate_answer(question, enhanced_context)
    
    # 5. 幻觉检测
    confidence_info = {}
    if return_confidence:
        confidence_info = self.hallucination_detector.detect_hallucination(
            answer, question, vector_results, graph_context
        )
    
    # 6. 构建响应
    response = {
        'answer': answer,
        'confidence': confidence_info['confidence'],
        'risk_level': confidence_info['risk_level'],
        'sources': self._build_sources_info(vector_results, graph_context),
        'processing_time': round(time.time() - start_time, 2)
    }
    return response
```
此过程将多种检索结果融合，注入到生成模型的提示词中，并通过幻觉检测确保了回答的准确性和可靠性。

**本节来源**
- [api_server.py](file://backend/api_server.py#L300-L876)
- [graphrag_engine.py](file://backend/graphrag_engine.py#L0-L485)

### 模糊查询与多轮对话分析
- **模糊查询与同义词扩展**：系统现在支持基于向量数据库的语义级模糊匹配。`VectorRetriever`类使用Ollama的`bge-m3:latest`嵌入模型将文本转换为向量，并在ChromaDB中进行相似度搜索，能有效处理同义词和近义词的语义匹配。
- **多轮对话上下文保持**：系统已实现多轮对话上下文记忆功能。`ConversationManager`类负责管理会话，`session_manager.py`文件中定义了`ConversationSession`对象来存储对话历史。每次`/api/ask`请求可以携带`session_id`，后端会将用户问题和系统回答添加到该会话的历史记录中，并在生成新回答时，将最近的对话历史作为上下文注入提示词。

**本节来源**
- [api_server.py](file://backend/api_server.py#L300-L876)
- [session_manager.py](file://backend/session_manager.py#L0-L412)
- [graphrag_engine.py](file://backend/graphrag_engine.py#L0-L485)

## 依赖分析
项目依赖通过`requirements.txt`文件管理，关键依赖如下：
- `flask` 和 `flask-cors`：构建后端Web API。
- `neo4j`：与Neo4j图数据库进行交互。
- `ollama`：调用本地或远程的Ollama大模型服务。
- `python-dotenv`：加载环境变量配置。
- `chromadb`：作为向量数据库，存储和检索文本嵌入。
- `sentence-transformers`：作为Ollama嵌入服务的备用方案。

``mermaid
graph TD
A[api_server.py] --> B[Flask]
A --> C[Neo4j Driver]
A --> D[Ollama Client]
A --> E[python-dotenv]
A --> F[ChromaDB]
A --> G[sentence-transformers]
B --> H[Web Server]
C --> I[Neo4j Database]
D --> J[Ollama Service]
F --> K[向量数据库]
G --> L[本地嵌入模型]
```

**图示来源**
- [requirements.txt](file://requirements.txt#L1-L7)
- [api_server.py](file://backend/api_server.py#L0-L119)

**本节来源**
- [requirements.txt](file://requirements.txt#L1-L7)
- [api_server.py](file://backend/api_server.py#L0-L119)

## 性能考虑
通过对代码和文档的分析，当前系统已实现多种性能优化机制。
- **潜在瓶颈**：每次用户提问都会触发一次向量数据库查询、一次图数据库查询和一次大模型调用。
- **优化建议**：
    1.  **引入缓存层**：对于相同的或高度相似的问题，可以直接返回缓存的答案，避免重复的数据库查询和大模型调用。
    2.  **向量数据库集成**：已通过`vector_retrieval.py`中的`VectorRetriever`类集成ChromaDB向量数据库，将政策法规文本转换为向量，并进行语义相似度搜索，显著提升了检索的准确性和召回率。
    3.  **查询优化**：在Neo4j中为`Policy.title`、`Section.title`等常用查询字段创建索引，以加速`WHERE`子句的执行。
    4.  **并行处理**：GraphRAG引擎并行执行向量检索和图谱查询，减少了整体响应时间。

**本节来源**
- [README.md](file://README.md#L13-L229)
- [api_server.py](file://backend/api_server.py#L0-L119)
- [vector_retrieval.py](file://backend/vector_retrieval.py#L18-L379)

## 故障排除指南
- **前端无法连接后端**：检查后端服务是否已启动（`python backend/api_server.py`），并确认前端代码中的`backendUrl`地址是否正确。
- **后端无法连接Neo4j**：确认`NEO4J_URI`、`NEO4J_USERNAME`和`NEO4J_PASSWORD`环境变量配置正确，并确保Neo4j服务正在运行。可运行`scripts/test_neo4j_connection.py`脚本进行测试。
- **后端无法调用Ollama**：确认`LLM_BINDING_HOST`指向正确的Ollama服务地址，并确保Ollama服务已启动。可运行`scripts/test_ollama_connection.py`脚本进行测试。特别注意，系统已强制配置为使用远程Ollama服务（`http://120.232.79.82:11434`），避免连接本地服务。
- **返回“未找到相关信息”**：检查`database`目录下的JSON数据是否已成功导入Neo4j，并确认问题中的关键词与政策文本中的表述一致。
- **向量数据库初始化失败**：检查`CHROMA_PERSIST_DIR`环境变量指向的目录是否存在且可写。

**本节来源**
- [README.md](file://README.md#L13-L229)
- [test_neo4j_connection.py](file://scripts/test_neo4j_connection.py#L0-L24)
- [test_ollama_connection.py](file://scripts/test_ollama_connection.py#L0-L24)
- [vector_retrieval.py](file://backend/vector_retrieval.py#L18-L379)